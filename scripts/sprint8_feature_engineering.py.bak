#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Sprint 8 – Feature Engineering (FULL)
- Robust Loader: searches data roots (raw/assembled/aggregates)
- Symbol normalization hardened (no 'NAN' string; fallback 'UNKNOWN')
- Demo mode (synthetic) when requested or no inputs found
- Feature sets: base, micro, regime
- Writes: output/features/{base,micro,regime}_<freq>.parquet + feature_manifest.json

Usage examples
--------------
python scripts/sprint8_feature_engineering.py --freq 5min --symbols AAPL,MSFT --quick
python scripts/sprint8_feature_engineering.py --freq 1min --demo --demo-days 30
"""
from __future__ import annotations
import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List

import numpy as np
import pandas as pd

ROOT = Path(__file__).resolve().parents[1]
DATA = ROOT / "data"
RAW_DIR = DATA / "raw"
OUT = ROOT / "output"
AGG = OUT / "aggregates"
ASM = OUT / "assembled_intraday"
FEAT = OUT / "features"
LOGS = ROOT / "logs"
for p in (FEAT, LOGS):
    p.mkdir(parents=True, exist_ok=True)

SEARCH_ROOTS = [RAW_DIR, ASM, AGG]
BASE_SCHEMA = ["timestamp", "symbol", "open", "high", "low", "close", "volume"]

# -------------------- utils & logging --------------------

def log(msg: str) -> None:
    ts = pd.Timestamp.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [SPRINT8] {msg}")

# -------------------- symbol normalization --------------------

def _normalize_symbol_str(s: str) -> str:
    s = s.strip().upper()
    if not s:
        return s
    # normalize delimiters
    s2 = s.replace("/", ".").replace("-", ".")
    # handle vendor prefixes like "XETRA:AAPL" (keep right side)
    if ":" in s2:
        s2 = s2.split(":")[-1]
    # drop right-side suffixes like ".US"
    if "." in s2:
        s2 = s2.split(".")[0]
    return s2

def _normalize_symbol_series(ser: pd.Series) -> pd.Series:
    ser = ser.copy()
    # keep NaN as NaN, do not cast to string first
    mask = ser.notna()
    ser.loc[mask] = ser.loc[mask].astype(str).map(_normalize_symbol_str)
    return ser

# -------------------- schema checks --------------------

def assert_schema(df: pd.DataFrame, required: Iterable[str], name: str) -> None:
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"[{name}] Missing columns: {missing}")

def assert_invariants(df: pd.DataFrame, name: str) -> None:
    if df.empty:
        raise ValueError(f"[{name}] DataFrame is empty")

# -------------------- data loading --------------------

def find_candidates(freq: str) -> List[Path]:
    patterns = ["*.parquet", "*.csv"]
    cands: List[Path] = []
    for root in SEARCH_ROOTS:
        p = (root / freq) if (root / freq).exists() else root
        for pat in patterns:
            cands.extend(p.rglob(pat))
    return sorted({c.resolve() for c in cands})

def read_any(path: Path) -> pd.DataFrame:
    if path.suffix.lower() == ".parquet":
        return pd.read_parquet(path)
    return pd.read_csv(path)

def load_raw(freq: str, symbols: List[str] | None, quick_days: int | None, allow_demo: bool, demo_days: int) -> pd.DataFrame:
    log(f"Symbols filter: {symbols if symbols else '[ALL]'}")
    sr = [str((r / freq).resolve()) if (r / freq).exists() else str(r.resolve()) for r in SEARCH_ROOTS]
    log(f"Search roots: {sr}")
    cands = [p for p in find_candidates(freq) if p.is_file()]
    log(f"Found {len(cands)} candidate file(s)")

    if not cands and allow_demo:
        log("No input files found — using synthetic demo data.")
        return make_demo(freq=freq, days=demo_days)
    elif not cands:
        raise FileNotFoundError(
            "No input files found. Looked in: " + ", ".join(str((r / freq).resolve()) for r in SEARCH_ROOTS)
        )

    frames: List[pd.DataFrame] = []
    for f in cands:
        df = read_any(f)
        # column aliasing
        aliases = {
            "time": "timestamp",
            "datetime": "timestamp",
            "ts": "timestamp",
            "ticker": "symbol",
            "secid": "symbol",
        }
        for a, b in aliases.items():
            if a in df.columns and b not in df.columns:
                df = df.rename(columns={a: b})
        assert_schema(df, BASE_SCHEMA, f"RAW:{f.name}")
        # timestamps → UTC naive
        df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True).dt.tz_convert(None)
        # normalize symbols robustly
        df["symbol"] = _normalize_symbol_series(df["symbol"]).astype(object)
        frames.append(df[BASE_SCHEMA])

    all_df = pd.concat(frames, ignore_index=True)
    assert_invariants(all_df, "RAW_ALL")

    # Quick window
    if quick_days is not None:
        cutoff = all_df["timestamp"].max() - pd.Timedelta(days=quick_days)
        before = len(all_df)
        all_df = all_df[all_df["timestamp"] >= cutoff]
        after = len(all_df)
        log(f"Quick filter: cutoff={cutoff} → rows {after} (from {before})")
        if after == 0 and allow_demo:
            log("Quick removed all rows → using demo data.")
            return make_demo(freq=freq, days=demo_days)

    # Symbol filter
    if symbols:
        before = len(all_df)
        filt = all_df["symbol"].isin([_normalize_symbol_str(s) for s in symbols])
        kept = all_df.loc[filt]
        log(f"Applied symbol filter {symbols} → rows {len(kept)} (from {before})")
        if kept.empty:
            log("WARNING: Symbol filter removed all rows → falling back to all symbols.")
        else:
            all_df = kept

    # Final fallback: if still NaNs in symbol, replace with 'UNKNOWN' (never 'NAN' string)
    if all_df["symbol"].isna().any():
        all_df["symbol"] = all_df["symbol"].astype(object)
        all_df["symbol"] = all_df["symbol"].where(all_df["symbol"].notna(), "UNKNOWN")

    return all_df.sort_values(["symbol", "timestamp"]).reset_index(drop=True)

# -------------------- demo data --------------------

def make_demo(freq: str, days: int = 30) -> pd.DataFrame:
    rng = np.random.default_rng(7)
    syms = ["AAPL", "MSFT"]
    # pick step based on freq
    step = dict(**{"1min": "1min", "5min": "5min", "15min": "15min"}).get(freq, "5min")
    end = pd.Timestamp.utcnow().floor("min")
    start = end - pd.Timedelta(days=days)
    idx = pd.date_range(start, end, freq=step, inclusive="left", tz="UTC").tz_convert(None)
    frames = []
    for s in syms:
        ret = rng.normal(0, 0.0009, len(idx))
        px = 100 * np.exp(np.cumsum(ret))
        op = px * (1 + rng.normal(0, 0.0003, len(idx)))
        hi = np.maximum.reduce([op, px, op * (1 + np.abs(rng.normal(0, 0.0007, len(idx))))])
        lo = np.minimum.reduce([op, px, op * (1 - np.abs(rng.normal(0, 0.0007, len(idx))))])
        vo = rng.integers(1_000, 50_000, len(idx)).astype(float)
        frames.append(pd.DataFrame({
            "timestamp": idx,
            "symbol": s,
            "open": op,
            "high": hi,
            "low": lo,
            "close": px,
            "volume": vo,
        }))
    return pd.concat(frames, ignore_index=True)

# -------------------- features --------------------

def features_base(df: pd.DataFrame, freq: str) -> pd.DataFrame:
    out = df.copy()
    out["return_1"] = out.groupby("symbol")["close"].pct_change()
    out["hl_spread"] = (out["high"] - out["low"]) / out["close"].replace(0, np.nan)
    out["vol_z"] = out.groupby("symbol")["volume"].transform(lambda s: (s - s.rolling(50).mean()) / (s.rolling(50).std() + 1e-9))
    return out

def features_micro(df: pd.DataFrame, freq: str) -> pd.DataFrame:
    out = df.copy()
    out["mom_3"] = out.groupby("symbol")["close"].pct_change(3)
    out["mom_12"] = out.groupby("symbol")["close"].pct_change(12)
    out["atr_14"] = (out["high"] - out["low"]).rolling(14).mean()
    out["spread_proxy"] = (out["high"] - out["low"]) / out["close"].replace(0, np.nan)
    return out

def features_regime(df: pd.DataFrame, freq: str) -> pd.DataFrame:
    out = df.copy()
    # Trend regime: sign of 12-period momentum
    tr = out.groupby("symbol")["close"].pct_change(12)
    out["trend_regime"] = np.sign(tr).fillna(0).astype(int)
    # Vol regime: high vol if rolling stdev above median
    vol = out.groupby("symbol")["return_1"].transform(lambda s: s.rolling(20).std())
    med = vol.groupby(out["symbol"]).transform("median")
    out["vol_regime"] = (vol > med).astype(int)
    # Liquidity regime: high if volume above rolling median
    vmed = out.groupby("symbol")["volume"].transform(lambda s: s.rolling(20).median())
    out["liq_regime"] = (out["volume"] >= vmed).fillna(False).astype(int)
    # Ensure dtypes small & safe
    out["vol_regime"] = out["vol_regime"].astype("int8")
    out["liq_regime"] = out["liq_regime"].astype("int8")
    out["trend_regime"] = out["trend_regime"].astype("int8")
    return out

# -------------------- manifest --------------------

def write_manifest(freq: str, written: List[str]) -> None:
    man = {
        "freq": freq,
        "written": written,
        "timestamp": pd.Timestamp.utcnow().isoformat() + "Z",
    }
    (FEAT / "feature_manifest.json").write_text(json.dumps(man, indent=2), encoding="utf-8")

# -------------------- main --------------------

def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--freq", default="1min", choices=["1min", "5min", "15min"])
    ap.add_argument("--symbols", default=None, help="Comma separated list or omit for all")
    ap.add_argument("--quick", action="store_true")
    ap.add_argument("--quick-days", type=int, default=180)
    ap.add_argument("--demo", action="store_true")
    ap.add_argument("--demo-days", type=int, default=30)
    args = ap.parse_args()

    log(f"START Sprint8 Feature Build | freq={args.freq} quick={args.quick} qdays={args.quick_days}")
    symbols = [s.strip() for s in args.symbols.split(",")] if args.symbols else None

    df = load_raw(
        freq=args.freq,
        symbols=symbols,
        quick_days=(args.quick_days if args.quick else None),
        allow_demo=args.demo,
        demo_days=args.demo_days,
    )

    log(f"Loaded rows={len(df)} | symbols={sorted(df['symbol'].dropna().unique().tolist())} | timerange=[{df['timestamp'].min()}, {df['timestamp'].max()}]")

    base = features_base(df, args.freq)
    micro = features_micro(base, args.freq)
    regime = features_regime(micro, args.freq)

    # Write outputs
    out_base = FEAT / f"base_{args.freq}.parquet"
    out_micro = FEAT / f"micro_{args.freq}.parquet"
    out_reg = FEAT / f"regime_{args.freq}.parquet"
    base.to_parquet(out_base, index=False)
    micro.to_parquet(out_micro, index=False)
    regime.to_parquet(out_reg, index=False)

    log(f"[OK] written: {out_base}")
    log(f"[OK] written: {out_micro}")
    log(f"[OK] written: {out_reg}")

    write_manifest(args.freq, [str(out_base), str(out_micro), str(out_reg)])
    log("Acceptance hint: Validate PF>1.25 under cost stress once Execution v2 is wired.")
    log("DONE Sprint8 Feature Build")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())
