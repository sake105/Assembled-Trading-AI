# scripts/run_daily.py
"""EOD-MVP Runner: Daily order generation using core data/features/signals/portfolio/execution layers.

This script is the EOD-MVP (Minimum Viable Product) for daily order generation.
It uses the new modular layers from Phase 3:
- data.prices_ingest: Load EOD prices with OHLCV
- features.ta_features: Compute technical indicators
- signals.rules_trend: Generate trend-following signals
- portfolio.position_sizing: Determine target positions
- execution.order_generation: Generate orders from targets
- execution.safe_bridge: Write SAFE-Bridge compatible CSV files

Difference from run_eod_pipeline.py:
- run_eod_pipeline.py: Full pipeline with backtest, portfolio simulation, QA
- run_daily.py: Focused EOD-MVP that only generates SAFE orders (no backtest/portfolio equity)

Note on Source Tagging (Phase 10):
    Orders generated by this script are currently written to SAFE-CSV files.
    If orders are later routed to the Paper-Trading-API, the source field should
    be set to "CLI_EOD" to identify the origin (e.g., in PaperOrderRequest.source).
"""

from __future__ import annotations

import argparse
import sys
from contextlib import nullcontext
from datetime import datetime
from pathlib import Path

import pandas as pd

# Import core modules
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

from src.assembled_core.config.settings import get_settings
from src.assembled_core.data.prices_ingest import (
    load_eod_prices,
    load_eod_prices_for_universe,
)
from src.assembled_core.execution.safe_bridge import write_safe_orders_csv
from src.assembled_core.logging_utils import setup_logging
from src.assembled_core.pipeline.trading_cycle import (
    TradingContext,
    run_trading_cycle,
)
from src.assembled_core.portfolio.position_sizing import (
    compute_target_positions_from_trend_signals,
)
from src.assembled_core.signals.rules_trend import generate_trend_signals_from_prices
from src.assembled_core.utils.timing import timed_step, write_timings_json


def parse_target_date(date_str: str | None) -> datetime:
    """Parse target date string.

    Args:
        date_str: Date string (YYYY-MM-DD) or None for today

    Returns:
        datetime object (UTC, time set to 00:00:00)

    Raises:
        ValueError: If date string format is invalid
    """
    if date_str:
        try:
            target_date = datetime.strptime(date_str, "%Y-%m-%d")
            # Ensure UTC timezone
            if target_date.tzinfo is None:
                target_date = target_date.replace(tzinfo=pd.Timestamp.utcnow().tz)
            return target_date
        except ValueError:
            raise ValueError(f"Invalid date format: {date_str}. Use YYYY-MM-DD")
    else:
        return datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)


def filter_prices_for_date(
    prices: pd.DataFrame, target_date: datetime, mode: str = "last_available"
) -> pd.DataFrame:
    """Filter prices for a specific date.

    This function filters price data based on the target date.

    **Date Filtering Modes:**
    - `"last_available"` (default): Returns data up to and including target_date.
      For each symbol, uses the last available timestamp <= target_date.
      This is useful for EOD runs where we want the most recent data available.
    - `"exact"`: Returns only data exactly matching target_date (may be empty if no data for that day).

    Args:
        prices: DataFrame with columns: timestamp, symbol, ...
        target_date: Target date (datetime, UTC)
        mode: Filtering mode ("last_available" or "exact"), default: "last_available"

    Returns:
        Filtered DataFrame with data for the target date (or last available before/on target_date)
    """
    if prices.empty:
        return prices

    # Ensure timestamp is timezone-aware UTC
    if prices["timestamp"].dt.tz is None:
        prices = prices.copy()
        prices["timestamp"] = pd.to_datetime(prices["timestamp"], utc=True)

    if mode == "exact":
        # Filter to exact date (may be empty)
        target_date_only = target_date.date()
        prices["date_only"] = pd.to_datetime(prices["timestamp"]).dt.date
        filtered = prices[prices["date_only"] == target_date_only].copy()
        filtered = filtered.drop(columns=["date_only"])
    else:  # mode == "last_available"
        # For each symbol, get the last available timestamp <= target_date
        filtered_list = []
        for symbol in prices["symbol"].unique():
            sym_data = prices[prices["symbol"] == symbol].copy()
            sym_data = sym_data[sym_data["timestamp"] <= target_date]
            if not sym_data.empty:
                # Get last available timestamp for this symbol
                last_ts = sym_data["timestamp"].max()
                last_row = sym_data[sym_data["timestamp"] == last_ts]
                filtered_list.append(last_row)

        if filtered_list:
            filtered = pd.concat(filtered_list, ignore_index=True)
        else:
            filtered = pd.DataFrame(columns=prices.columns)

    return filtered


def validate_universe_vs_data(
    universe_symbols: list[str], prices: pd.DataFrame, target_date: datetime
) -> tuple[pd.DataFrame, list[str]]:
    """Validate that universe symbols have price data and filter out missing ones.

    Args:
        universe_symbols: List of symbols from universe file
        prices: DataFrame with price data (columns: timestamp, symbol, ...)
        target_date: Target date for filtering

    Returns:
        Tuple of (filtered_prices, missing_symbols)
        filtered_prices: Price data only for symbols that have data
        missing_symbols: List of symbols from universe that have no data
    """
    if prices.empty:
        return prices, universe_symbols

    # Filter prices to target date (last available)
    prices_filtered = filter_prices_for_date(prices, target_date, mode="last_available")

    if prices_filtered.empty:
        return prices_filtered, universe_symbols

    # Get symbols that have data
    available_symbols = set(prices_filtered["symbol"].str.upper().unique())
    universe_symbols_upper = [s.upper() for s in universe_symbols]

    # Find missing symbols
    missing_symbols = [s for s in universe_symbols_upper if s not in available_symbols]

    # Filter prices to only universe symbols that have data
    if available_symbols:
        valid_symbols = [s for s in universe_symbols_upper if s in available_symbols]
        if valid_symbols:
            prices_filtered = prices_filtered[
                prices_filtered["symbol"].str.upper().isin(valid_symbols)
            ].copy()
        else:
            prices_filtered = pd.DataFrame(columns=prices_filtered.columns)

    return prices_filtered, missing_symbols


def run_daily_eod(
    date_str: str | None = None,
    universe_file: Path | str | None = None,
    price_file: Path | str | None = None,
    output_dir: Path | str | None = None,
    total_capital: float = 1.0,
    top_n: int | None = None,
    ma_fast: int = 20,
    ma_slow: int = 50,
    min_score: float = 0.0,
    disable_pre_trade_checks: bool = False,
    ignore_kill_switch: bool = False,
    enable_timings: bool = False,
    timings_out: Path | str | None = None,
    use_factor_store: bool = False,
    factor_store_root: Path | str | None = None,
    factor_group: str = "core_ta",
) -> Path:
    """Run daily EOD order generation.

    **Date Handling:**
    The `date_str` parameter specifies the target trading day for which orders should be generated.
    Price data is filtered to the last available data <= target_date (per symbol).
    This ensures that even if data for the exact target date is missing, we use the most recent
    available data up to that date.

    **Universe vs. Data:**
    If universe_file is provided, symbols from the universe are validated against available price data.
    Symbols without data are logged as warnings and dropped from the flow.
    If no symbols remain after filtering, the script exits cleanly with a clear message.

    Args:
        date_str: Date string (YYYY-MM-DD) or None for today
        universe_file: Path to universe file (default: watchlist.txt)
        price_file: Optional explicit path to price file
        output_dir: Output directory (default: config.OUTPUT_DIR)
        total_capital: Total capital for position sizing (default: 1.0)
        top_n: Optional maximum number of positions (default: None = all)
        ma_fast: Fast moving average window (default: 20)
        ma_slow: Slow moving average window (default: 50)
        min_score: Minimum signal score threshold (default: 0.0)

    Returns:
        Path to generated SAFE orders CSV file

    Raises:
        FileNotFoundError: If price data or universe file not found
        ValueError: If date string is invalid or no valid symbols remain
        SystemExit: If no orders can be generated (no valid symbols)
    """
    # Setup logging
    logger = setup_logging(level="INFO")

    # Initialize timing dictionary (always initialized, but only populated if enabled)
    timings: dict = {}

    # Parse date
    try:
        target_date = parse_target_date(date_str)
    except ValueError as e:
        logger.error(f"Invalid date format: {e}")
        sys.exit(1)

    # Determine output directory (use settings if not provided)
    if output_dir:
        out_dir = Path(output_dir)
    else:
        settings = get_settings()
        out_dir = settings.output_dir
    out_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"Starting EOD-MVP for {target_date.strftime('%Y-%m-%d')}")
    logger.info(f"Output directory: {out_dir}")

    # Step 1: Load universe symbols (if universe_file provided, else use default from settings)
    universe_symbols = []
    if universe_file:
        universe_path = Path(universe_file)
    else:
        # Use default watchlist from settings
        settings = get_settings()
        universe_path = settings.watchlist_file

    if universe_path and universe_path.exists():
        # Read symbols from universe file
        try:
            with open(universe_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        universe_symbols.append(line.upper())

            if not universe_symbols:
                logger.error(f"No symbols found in universe file: {universe_path}")
                sys.exit(1)

            logger.info(f"Universe loaded: {len(universe_symbols)} symbols")
        except Exception as e:
            logger.error(f"Failed to read universe file: {e}", exc_info=True)
            sys.exit(1)

    # Step 2: Load EOD prices
    logger.info("Step 1: Loading EOD prices...")
    try:
        step_name = "load_data"
        step_context = timed_step(step_name, timings, logger) if enable_timings else nullcontext()
        with step_context:
            if price_file:
                price_path = Path(price_file)
                if not price_path.exists():
                    logger.error(f"Price file not found: {price_path}")
                    sys.exit(1)

                prices = load_eod_prices(price_file=price_path, freq="1d")
            else:
                prices = load_eod_prices_for_universe(
                    universe_file=universe_file, freq="1d"
                )

            if prices.empty:
                logger.error("Price data is empty after loading")
                sys.exit(1)

            logger.info(
                f"Loaded prices: {len(prices)} rows, {prices['symbol'].nunique()} symbols"
            )
    except FileNotFoundError as e:
        logger.error(f"Price file not found: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Failed to load prices: {e}", exc_info=True)
        sys.exit(1)

    # Step 3: Validate universe vs. data and filter
    if universe_symbols:
        logger.info("Step 2: Validating universe symbols against price data...")
        prices_filtered, missing_symbols = validate_universe_vs_data(
            universe_symbols, prices, target_date
        )

        if missing_symbols:
            logger.warning(
                f"{len(missing_symbols)} symbols from universe have no price data: {', '.join(missing_symbols)}"
            )
            logger.warning("These symbols will be dropped from the flow.")

        if prices_filtered.empty:
            logger.error("No valid symbols with price data remain after filtering.")
            logger.error("No orders will be generated.")
            sys.exit(1)

        prices = prices_filtered
        logger.info(f"Valid symbols: {prices['symbol'].nunique()} symbols with data")
    else:
        # No universe file: filter prices to target date
        logger.info("Step 2: Filtering prices to target date...")
        prices = filter_prices_for_date(prices, target_date, mode="last_available")

        if prices.empty:
            logger.error(
                f"No price data available for target date {target_date.strftime('%Y-%m-%d')}"
            )
            sys.exit(1)

        logger.info(
            f"Filtered prices: {len(prices)} rows, {prices['symbol'].nunique()} symbols"
        )

    # Step 3.5: Run QC checks and set QA Gate (Sprint 3 / D2)
    qa_block_trading = False
    qa_block_reason = None
    try:
        from src.assembled_core.qa.data_qc import run_price_panel_qc, write_qc_report_json
        
        logger.info("Step 2.5: Running data quality control (QC)...")
        # Note: target_timestamp is defined later, so we use None here
        # (QC will use prices timestamp range for missing sessions check)
        qc_report = run_price_panel_qc(
            prices=prices,
            freq="1d",
            calendar="NYSE",
            as_of=None,  # Will be set later when target_timestamp is available
        )
        
        # Write QC report to output directory
        qc_report_path = out_dir / "qc_report.json"
        write_qc_report_json(qc_report, qc_report_path)
        logger.info(f"QC report written: {qc_report_path}")
        
        # Set QA Gate if QC has FAIL issues
        if not qc_report.ok:
            qa_block_trading = True
            qa_block_reason = f"DATA_QC_FAIL: {qc_report.summary.get('fail_count', 0)} FAIL issues, {qc_report.summary.get('warn_count', 0)} WARN issues"
            logger.warning(f"QC FAILED: {qa_block_reason}")
            logger.warning("Trading will be blocked (no orders generated)")
        elif qc_report.summary.get("warn_count", 0) > 0:
            logger.warning(f"QC WARN: {qc_report.summary.get('warn_count', 0)} WARN issues (trading will proceed)")
    except ImportError:
        logger.warning(
            "QC module not available - skipping QC checks. "
            "Install exchange-calendars for QC support."
        )
    except Exception as e:
        logger.warning(f"QC check failed: {e} - proceeding without QC")
        logger.debug(f"QC error details: {e}", exc_info=True)

    # Step 4-7: Run unified trading cycle
    logger.info("Step 3: Running trading cycle (features/signals/positions/orders/risk)...")
    try:
        step_name = "trading_cycle"
        step_context = timed_step(step_name, timings, logger) if enable_timings else nullcontext()
        
        with step_context:
            # Build TradingContext from CLI args
            # Convert target_date (datetime) to pd.Timestamp for TradingContext
            target_timestamp = pd.Timestamp(target_date) if isinstance(target_date, datetime) else target_date
            
            # Normalize as_of to session close for 1d frequency (Sprint-2 calendar hardening)
            try:
                from src.assembled_core.data.calendar import normalize_as_of_to_session_close
                target_timestamp = normalize_as_of_to_session_close(target_timestamp)
                logger.debug(f"Normalized as_of to session close: {target_timestamp}")
            except ImportError:
                # If exchange_calendars not installed, log warning but continue
                logger.warning(
                    "exchange_calendars not installed - as_of not normalized to session close. "
                    "Install with: pip install exchange-calendars"
                )
            except ValueError as e:
                # If date is not a trading day, log error and exit
                logger.error(f"Failed to normalize as_of to session close: {e}")
                sys.exit(1)
            
            # Define signal function
            def signal_fn(df: pd.DataFrame) -> pd.DataFrame:
                return generate_trend_signals_from_prices(df, ma_fast=ma_fast, ma_slow=ma_slow)
            
            # Define position sizing function
            def sizing_fn(signals: pd.DataFrame, capital: float) -> pd.DataFrame:
                return compute_target_positions_from_trend_signals(
                    signals, total_capital=capital, top_n=top_n, min_score=min_score
                )
            
            # Build TradingContext
            ctx = TradingContext(
                prices=prices,  # Full prices (will be filtered by trading_cycle)
                as_of=target_timestamp,
                freq="1d",
                universe=universe_symbols if universe_symbols else None,
                use_factor_store=use_factor_store,
                factor_store_root=Path(factor_store_root) if factor_store_root else None,
                factor_group=factor_group,
                feature_config={
                    "ma_windows": (ma_fast, ma_slow),
                    "atr_window": 14,
                    "rsi_window": 14,
                    "include_rsi": True,
                },
                signal_fn=signal_fn,
                signal_config={"ma_fast": ma_fast, "ma_slow": ma_slow},
                position_sizing_fn=sizing_fn,
                capital=total_capital,
                current_positions=None,  # No current positions in EOD flow
                order_timestamp=target_timestamp,
                enable_risk_controls=not disable_pre_trade_checks and not ignore_kill_switch,
                risk_config={
                    "enable_pre_trade_checks": not disable_pre_trade_checks,
                    "enable_kill_switch": not ignore_kill_switch,
                },
                output_dir=out_dir,
                output_format="none",  # Don't write outputs in trading_cycle (we do it manually)
                write_outputs=False,  # Pure function
                run_id=None,  # No run_id in EOD flow
                strategy_name="EOD Strategy - Daily MVP",
                logger=logger,
                timings=timings if enable_timings else None,
            )
            
            # Run trading cycle
            result = run_trading_cycle(ctx)
            
            if result.status != "success":
                logger.error(f"Trading cycle failed: {result.error_message}")
                sys.exit(1)
            
            # Extract results
            orders = result.orders_filtered  # Already filtered by risk controls
            
            # Log intermediate results
            if not result.signals.empty:
                long_signals = result.signals[result.signals["direction"] == "LONG"]
                logger.info(
                    f"Signals generated: {len(long_signals)} LONG signals from {len(result.signals)} total"
                )
            
            if not result.target_positions.empty:
                logger.info(f"Target positions: {len(result.target_positions)} symbols")
                logger.info(f"Symbols: {', '.join(result.target_positions['symbol'].tolist())}")
            else:
                logger.warning(
                    "No target positions computed (no LONG signals or all filtered out)"
                )
                logger.warning("No orders will be generated.")
                # Create empty SAFE file
                safe_path = write_safe_orders_csv(
                    pd.DataFrame(columns=["timestamp", "symbol", "side", "qty", "price"]),
                    date=target_date,
                    output_path=None,
                    price_type="MARKET",
                    comment="EOD Strategy - Daily MVP (no signals)",
                )
                logger.info(f"Empty SAFE orders file written: {safe_path}")
                return safe_path
            
            if orders.empty:
                logger.warning("No orders generated (no position changes or all filtered by risk controls)")
                # Create empty SAFE file
                safe_path = write_safe_orders_csv(
                    orders,
                    date=target_date,
                    output_path=None,
                    price_type="MARKET",
                    comment="EOD Strategy - Daily MVP (no orders)",
                )
                logger.info(f"Empty SAFE orders file written: {safe_path}")
                return safe_path
            
            logger.info(f"Orders generated: {len(orders)} orders")
            if not orders.empty:
                buy_count = len(orders[orders["side"] == "BUY"])
                sell_count = len(orders[orders["side"] == "SELL"])
                logger.info(f"Order breakdown: {buy_count} BUY, {sell_count} SELL")
            
            # Store factor metadata in timings if enabled
            if enable_timings and result.meta:
                if step_name in timings:
                    if "meta" not in timings[step_name]:
                        timings[step_name]["meta"] = {}
                    timings[step_name]["meta"].update(result.meta)
                    
    except Exception as e:
        logger.error(f"Failed to run trading cycle: {e}", exc_info=True)
        sys.exit(1)

    # Step 8: Write SAFE-Bridge CSV
    logger.info("Step 4: Writing SAFE-Bridge CSV...")
    try:
        step_name = "outputs"
        step_context = timed_step(step_name, timings, logger) if enable_timings else nullcontext()
        with step_context:
            safe_path = write_safe_orders_csv(
                orders,
                date=target_date,
                output_path=None,  # Use default: output/orders_YYYYMMDD.csv
                price_type="MARKET",
                comment="EOD Strategy - Daily MVP",
            )
            logger.info(f"SAFE orders written: {safe_path}")
            logger.info(f"Total orders: {len(orders)}")
    except ValueError as e:
        # ValueError from validation - log clearly and exit
        error_msg = str(e)
        if "SAFE orders validation failed" in error_msg:
            logger.error(
                f"SAFE orders validation failed - no file written. {error_msg}"
            )
        else:
            logger.error(f"Failed to write SAFE orders: {error_msg}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Failed to write SAFE orders: {e}", exc_info=True)
        sys.exit(1)

    # Summary log
    unique_symbols = orders["symbol"].nunique() if not orders.empty else 0
    buy_count = len(orders[orders["side"] == "BUY"]) if not orders.empty else 0
    sell_count = len(orders[orders["side"] == "SELL"]) if not orders.empty else 0
    logger.info(
        f"Summary: {unique_symbols} symbols, {len(orders)} orders ({buy_count} BUY, {sell_count} SELL), file: {safe_path.name}"
    )

    logger.info(f"SUCCESS: EOD-MVP completed for {target_date.strftime('%Y-%m-%d')}")

    # Write timings if enabled
    if enable_timings:
        if timings_out:
            timings_path = Path(timings_out)
        else:
            # Default: use run_timings.json (or timings.json for backward compatibility)
            timings_path = out_dir / "run_timings.json"
        
        job_meta = {
            "date": target_date.strftime("%Y-%m-%d"),
            "universe_file": str(universe_file) if universe_file else None,
            "total_capital": total_capital,
            "ma_fast": ma_fast,
            "ma_slow": ma_slow,
        }
        write_timings_json(timings, timings_path, job_name="run_daily", job_meta=job_meta)

    return safe_path


def main() -> None:
    """CLI entry point for daily EOD runner."""
    p = argparse.ArgumentParser(
        description="EOD-MVP Runner: Generate daily SAFE orders using core layers"
    )
    p.add_argument(
        "--date",
        type=str,
        default=None,
        help="Date (YYYY-MM-DD), default: today. Orders are generated for this trading day using the last available price data <= date.",
    )
    p.add_argument(
        "--universe",
        type=str,
        default=None,
        help="Path to universe file (default: from settings.watchlist_file, typically watchlist.txt)",
    )
    p.add_argument(
        "--price-file",
        type=str,
        default=None,
        help="Optional explicit path to price file",
    )
    p.add_argument(
        "--out",
        type=str,
        default=None,
        help="Output directory (default: from settings.output_dir)",
    )
    p.add_argument(
        "--total-capital",
        type=float,
        default=1.0,
        help="Total capital for position sizing (default: 1.0)",
    )
    p.add_argument(
        "--top-n",
        type=int,
        default=None,
        help="Maximum number of positions (default: None = all)",
    )
    p.add_argument(
        "--ma-fast",
        type=int,
        default=20,
        help="Fast moving average window (default: 20)",
    )
    p.add_argument(
        "--ma-slow",
        type=int,
        default=50,
        help="Slow moving average window (default: 50)",
    )
    p.add_argument(
        "--min-score",
        type=float,
        default=0.0,
        help="Minimum signal score threshold (default: 0.0)",
    )
    p.add_argument(
        "--disable-pre-trade-checks",
        action="store_true",
        default=False,
        help="Disable pre-trade checks (only for debugging/backtesting, default: checks enabled)",
    )
    p.add_argument(
        "--ignore-kill-switch",
        action="store_true",
        default=False,
        help="Ignore kill switch (only for offline backtests/dev, default: kill switch respected)",
    )
    p.add_argument(
        "--enable-timings",
        action="store_true",
        default=False,
        help="Enable timing logs for each pipeline step (default: disabled)",
    )
    p.add_argument(
        "--timings-out",
        type=str,
        default=None,
        help="Path to write timings JSON file (default: output_dir/timings.json)",
    )
    p.add_argument(
        "--use-factor-store",
        action="store_true",
        default=False,
        help="Use factor store for caching (default: disabled, uses direct computation)",
    )
    p.add_argument(
        "--factor-store-root",
        type=str,
        default=None,
        help="Root directory for factor store (default: from settings or data/factors)",
    )
    p.add_argument(
        "--factor-group",
        type=str,
        default="core_ta",
        help="Factor group name for factor store (default: core_ta)",
    )

    args = p.parse_args()

    # Setup logging for main
    logger = setup_logging(level="INFO")

    try:
        safe_path = run_daily_eod(
            date_str=args.date,
            universe_file=Path(args.universe) if args.universe else None,
            price_file=Path(args.price_file) if args.price_file else None,
            output_dir=Path(args.out) if args.out else None,
            total_capital=args.total_capital,
            top_n=args.top_n,
            ma_fast=args.ma_fast,
            ma_slow=args.ma_slow,
            min_score=args.min_score,
            disable_pre_trade_checks=args.disable_pre_trade_checks,
            ignore_kill_switch=args.ignore_kill_switch,
            enable_timings=args.enable_timings,
            timings_out=Path(args.timings_out) if args.timings_out else None,
            use_factor_store=args.use_factor_store,
            factor_store_root=Path(args.factor_store_root) if args.factor_store_root else None,
            factor_group=args.factor_group,
        )

        logger.info(f"Output file: {safe_path}")
        sys.exit(0)

    except SystemExit:
        # Re-raise SystemExit (from sys.exit() calls)
        raise
    except Exception as e:
        logger.error(f"FATAL ERROR: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
