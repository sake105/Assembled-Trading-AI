# P3 - Backtest Engine Optimization (Design)

## 1. Overview & Scope P3

The goal of P3 is to significantly accelerate backtest execution while preserving exact numerical results and maintaining API compatibility with the existing backtest engine.

**Relationship to P1/P2:**
- **P1 (Performance Profiling)**: Established baseline measurements and identified hotspots
- **P2 (Factor Store)**: Reduced factor computation overhead by enabling fast factor panel loading
- **P3 (Backtest Optimization)**: Targets the backtest engine itself - the core execution loop that processes orders and updates equity

**Scope:**
- Vectorize equity updates, PnL calculations, and exposure computations
- Accelerate tight loops with Numba JIT compilation where vectorization is not clean
- Maintain 100% numerical compatibility with existing implementation (within floating-point tolerance)
- Preserve existing API: `run_portfolio_backtest()` signature and `BacktestResult` structure unchanged

**Out of Scope for P3:**
- Parallelization across multiple backtests (deferred to P4)
- Changes to signal generation or position sizing logic (those remain as-is)
- Database or distributed storage optimizations

---

## 2. Current State: Backtest Engine Analysis

### 2.1. Main Execution Flow

The current backtest engine (`src/assembled_core/qa/backtest_engine.py`) follows this structure:

1. **Input Processing:**
   - Load prices DataFrame (timestamp, symbol, close, ...)
   - Load signals DataFrame (timestamp, symbol, signal/score)
   - Convert signals to target positions via position sizing function

2. **Main Loop (Per-Day Iteration):**
   - Iterate over unique timestamps (sorted)
   - For each date:
     - Get current positions (from previous day)
     - Get target positions (from signals for this date)
     - Compute order list (target - current)
     - Execute orders (update positions, track trades)
     - Update equity (mark-to-market current positions)
     - Store equity, positions, trades for this date

3. **Post-Processing:**
   - Aggregate trades DataFrame
   - Compute metrics from equity curve
   - Return `BacktestResult` object

### 2.2. Identified Hotspots

**Primary Bottlenecks (from P1 profiling):**

1. **Per-Day Loop:**
   - Iteration over timestamps (typically 1000-5000 days for multi-year backtests)
   - For each day: multiple DataFrame operations (filtering, merging, indexing)

2. **Order Execution:**
   - Per-order processing (BUY/SELL logic)
   - Position updates (dictionary or DataFrame row updates)
   - Trade recording (append to list or DataFrame)

3. **Equity Update:**
   - Mark-to-market calculation: `equity = cash + sum(positions * current_prices)`
   - Currently computed per-day via DataFrame operations
   - Can be vectorized across all symbols at once

4. **Exposure Calculations:**
   - Gross/net exposure, HHI, turnover
   - Currently computed via per-day aggregations
   - Can be vectorized as post-processing step

**Secondary Bottlenecks:**

- Position DataFrame updates (row-by-row modifications)
- Trade DataFrame construction (repeated appends)
- Cost calculations (commission, spread, slippage) - currently per-trade

---

## 3. Optimization Goals & Priorities

### 3.1. Primary Targets (High Impact, Low Risk)

**Priority 1: Vectorize Equity Updates**
- Replace per-day equity calculation with vectorized operations
- Use `pandas` groupby/merge operations to compute equity for all days at once
- Expected speedup: 5-10x for equity computation

**Priority 2: Vectorize Order Execution**
- Batch process orders by date (group orders by timestamp)
- Use vectorized DataFrame operations for position updates
- Expected speedup: 3-5x for order processing

**Priority 3: Vectorize PnL & Exposure**
- Compute PnL, returns, exposure metrics as post-processing step
- Use vectorized operations on full equity/positions DataFrames
- Expected speedup: 2-3x for metrics computation

### 3.2. Secondary Targets (Medium Impact, Higher Risk)

**Priority 4: Numba-Accelerated Tight Loops**
- Identify loops that cannot be cleanly vectorized (e.g., order matching logic)
- Implement Numba JIT-compiled versions
- Fallback to pure Python if Numba not available
- Expected speedup: 2-4x for specific tight loops

**Priority 5: Optimize Trade Recording**
- Pre-allocate trade DataFrame instead of repeated appends
- Use list-of-dicts pattern, convert to DataFrame once at end
- Expected speedup: 1.5-2x for trade recording

### 3.3. Hotspots & Planned Optimization

| Hotspot | Current Approach | Planned Optimization | Expected Speedup | Risk |
|---------|------------------|---------------------|------------------|------|
| Equity update (per-day) | DataFrame operations in loop | Vectorized groupby/merge | 5-10x | Low |
| Order execution (per-order) | Row-by-row position updates | Batch vectorized updates | 3-5x | Medium |
| PnL/Exposure calculation | Per-day aggregations | Post-processing vectorization | 2-3x | Low |
| Trade recording | Repeated DataFrame appends | Pre-allocated list conversion | 1.5-2x | Low |
| Cost calculation (per-trade) | Per-trade function calls | Vectorized cost arrays | 2-3x | Low |
| Tight loops (order matching) | Pure Python loops | Numba JIT compilation | 2-4x | Medium |

---

## 4. Planned Technical Changes

### 4.1. Vectorized API Design

**New Functions (to be added to `backtest_engine.py`):**

```python
def compute_equity_vectorized(
    positions_df: pd.DataFrame,
    prices_df: pd.DataFrame,
    cash_series: pd.Series,
) -> pd.Series:
    """
    Vectorized equity computation for all timestamps at once.
    
    Args:
        positions_df: DataFrame with MultiIndex (timestamp, symbol), column: qty
        prices_df: DataFrame with columns: timestamp, symbol, close
        cash_series: Series with index=timestamp, values=cash
        
    Returns:
        Series with index=timestamp, values=equity
    """
    # Merge positions with prices
    # Compute position_values = positions * prices
    # Group by timestamp, sum position_values
    # Add cash to get equity
    pass

def execute_orders_vectorized(
    orders_df: pd.DataFrame,
    current_positions: pd.DataFrame,
    prices_df: pd.DataFrame,
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Vectorized order execution for a batch of orders.
    
    Args:
        orders_df: DataFrame with columns: timestamp, symbol, side, qty, price
        current_positions: DataFrame with MultiIndex (timestamp, symbol), column: qty
        prices_df: DataFrame with columns: timestamp, symbol, close
        
    Returns:
        Tuple of (updated_positions_df, trades_df)
    """
    # Group orders by timestamp
    # For each timestamp, batch update positions
    # Compute trades (executed orders)
    # Return updated positions and trades
    pass
```

**Integration Strategy:**
- Add vectorized functions alongside existing per-day logic
- Add feature flag or automatic detection (use vectorized if data size > threshold)
- Maintain existing API: `run_portfolio_backtest()` calls vectorized or non-vectorized path internally

### 4.2. Numba Integration Proposal

**Option A: Separate Module (Recommended)**
- Create `src/assembled_core/qa/backtest_engine_numba.py`
- Contains Numba-accelerated functions
- Optional import in `backtest_engine.py`:

```python
try:
    from src.assembled_core.qa.backtest_engine_numba import (
        compute_pnl_numba,
        match_orders_numba,
    )
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    # Fallback to pure Python implementations
```

**Option B: Inline Numba Functions**
- Add Numba-decorated functions directly in `backtest_engine.py`
- Use `@numba.jit` decorator with fallback

**Recommended: Option A** (cleaner separation, easier to test and maintain)

**Numba Functions (candidates):**
- `compute_pnl_numba(positions_array, prices_array, cash_array) -> equity_array`
- `match_orders_numba(orders_array, current_positions_array) -> updated_positions_array`
- `compute_costs_numba(trades_array, commission_bps, spread_bps) -> costs_array`

---

## 5. Compatibility & Testing Strategy

### 5.1. Numerical Compatibility

**Goal:** Results from optimized backtest must match existing implementation within floating-point tolerance.

**Tolerance Levels:**
- Equity values: `rtol=1e-9, atol=1e-9` (pandas default)
- Trade quantities/prices: `rtol=1e-9, atol=1e-9`
- Metrics (Sharpe, CAGR, etc.): `rtol=1e-6, atol=1e-6` (metrics have inherent precision limits)

**Validation Approach:**
- Run same backtest with both old and new implementation
- Compare equity curves, trades, and metrics
- Use `pd.testing.assert_frame_equal()` and `pd.testing.assert_series_equal()`

### 5.2. Regression Tests

**Test Suite (`tests/test_qa_backtest_engine_optimized.py`):**

1. **Numerical Compatibility Tests:**
   - `test_equity_vectorized_matches_original()`: Compare equity curves
   - `test_orders_vectorized_matches_original()`: Compare trades DataFrames
   - `test_metrics_match_original()`: Compare all metrics

2. **Edge Cases:**
   - Empty orders (no trades)
   - Single symbol backtest
   - Very large universe (100+ symbols)
   - Missing price data (NaN handling)

3. **Performance Benchmarks:**
   - `test_backtest_speedup()`: Measure speedup vs. original (target: 3-5x overall)
   - Benchmark scenarios: small (10 symbols, 1 year), medium (50 symbols, 5 years), large (100 symbols, 10 years)

### 5.3. Benchmark Scenarios

**Small Scenario:**
- 10 symbols, 252 trading days (1 year)
- Baseline target: < 1 second
- Optimized target: < 0.3 seconds

**Medium Scenario:**
- 50 symbols, 1260 trading days (5 years)
- Baseline target: < 10 seconds
- Optimized target: < 3 seconds

**Large Scenario:**
- 100 symbols, 2520 trading days (10 years)
- Baseline target: < 60 seconds
- Optimized target: < 15 seconds

---

## 6. Implementation Plan

### P3.1: Core Vectorization (Equity & PnL)

**Tasks:**
- Implement `compute_equity_vectorized()` function
- Replace per-day equity loop with vectorized computation
- Add feature flag `use_vectorized=True` to `run_portfolio_backtest()`
- Unit tests for vectorized equity computation
- Benchmark: compare speed vs. original

**Deliverables:**
- Vectorized equity computation working
- Tests passing (numerical compatibility verified)
- 5-10x speedup for equity updates

### P3.2: Vectorized Order Execution

**Tasks:**
- Implement `execute_orders_vectorized()` function
- Batch process orders by timestamp
- Vectorize position updates
- Integrate into main backtest loop
- Unit tests for order execution

**Deliverables:**
- Vectorized order execution working
- Tests passing (trades match original)
- 3-5x speedup for order processing

### P3.3: Numba Acceleration Layer

**Tasks:**
- Create `backtest_engine_numba.py` module
- Implement Numba-accelerated functions (PnL, order matching, costs)
- Add optional import with fallback
- Integrate Numba functions into vectorized path
- Unit tests for Numba functions

**Deliverables:**
- Numba module working (with fallback if not installed)
- Tests passing (numerical compatibility)
- 2-4x additional speedup for tight loops

### P3.4: Testing, Benchmarking & Documentation

**Tasks:**
- Comprehensive regression test suite
- Performance benchmarks (small/medium/large scenarios)
- Update documentation (API docs, performance notes)
- Integration tests with existing workflows
- Performance profiling (verify speedups match targets)

**Deliverables:**
- Full test coverage
- Benchmark results documented
- Documentation updated
- Ready for production use

---

## 7. Risks & Mitigation

### 7.1. Numerical Differences

**Risk:** Vectorized operations may introduce small numerical differences due to operation order changes.

**Mitigation:**
- Use same floating-point operations (just reordered)
- Validate with comprehensive test suite
- Document acceptable tolerance levels

### 7.2. Numba Dependency

**Risk:** Numba adds optional dependency, may not be available in all environments.

**Mitigation:**
- Make Numba optional (graceful fallback to pure Python)
- Document in requirements.txt as optional
- Test both paths (with and without Numba)

### 7.3. API Compatibility

**Risk:** Changes may break existing code that depends on internal backtest engine structure.

**Mitigation:**
- Preserve `run_portfolio_backtest()` signature exactly
- Preserve `BacktestResult` structure
- Internal optimizations only (no external API changes)

### 7.4. Memory Usage

**Risk:** Vectorized operations may use more memory (intermediate DataFrames).

**Mitigation:**
- Monitor memory usage in benchmarks
- Add memory-efficient alternatives for very large universes
- Document memory requirements

---

## 8. Success Criteria

**Performance Targets:**
- Overall backtest speedup: 3-5x for typical scenarios
- Equity computation: 5-10x faster
- Order execution: 3-5x faster
- Memory usage: < 2x increase

**Quality Targets:**
- 100% numerical compatibility (within tolerance)
- All existing tests pass
- No API changes
- Comprehensive test coverage (> 90%)

---

## 9. References

- [Performance Profiling P1 Design](PERFORMANCE_PROFILING_P1_DESIGN.md): Baseline measurements
- [Factor Store P2 Design](FACTOR_STORE_P2_DESIGN.md): Factor loading optimization
- Numba Documentation: https://numba.pydata.org/
- Pandas Performance: https://pandas.pydata.org/docs/user_guide/enhancingperf.html

---

## 10. Implementation Results (P3.1-P3.4)

### Status: âœ… Completed

P3 optimizations have been successfully implemented and tested. The backtest engine now uses vectorized operations and optional Numba acceleration while maintaining 100% numerical compatibility with the original implementation.

### Implemented Optimizations

**P3.1: Core Vectorization (Position Updates)**
- Implemented `_update_positions_vectorized()` function
- Replaced iterative `orders.iterrows()` loop with vectorized pandas operations
- Position delta computation uses `np.where()` instead of `apply()`
- Position aggregation uses pandas `groupby()` and `merge()`
- **Speedup**: 3-5x for position update operations

**P3.2: Performance Timing Integration**
- Added `timed_block` context managers for all major steps:
  - `backtest_step1_features`
  - `backtest_step2_signal_generation`
  - `backtest_step3_position_sizing`
  - `backtest_step4_equity_simulation`
  - `backtest_step5_equity_enhancement`
- Enables detailed performance profiling in production

**P3.3: Numba Acceleration Layer**
- Created `src/assembled_core/qa/backtest_engine_numba.py` module
- Implemented Numba-accelerated functions:
  - `compute_position_deltas_numba()`: JIT-compiled delta computation
  - `aggregate_position_deltas_numba()`: JIT-compiled symbol aggregation
- Optional integration with graceful fallback to pandas
- **Additional speedup**: 2-4x for tight loops (when Numba available)

**P3.4: Testing & Benchmarking**
- Comprehensive regression tests (`tests/test_performance_backtest_engine_regression.py`)
- Benchmark script (`scripts/benchmark_backtest_engine.py`)
- All tests pass (26 tests: 12 backtest-engine + 9 numba + 5 regression)
- Numerical compatibility verified (tolerance: 1e-9)

### Performance Improvements

**Measured Speedups** (example benchmark: 5 symbols, 63 days):
- Overall backtest: ~1.5-2x faster (position sizing step: largest improvement)
- Position updates: 3-5x faster (vectorization)
- Throughput: ~690 days/second (small universe benchmark)

**Key Hotspots Optimized:**
- Position update loop: Vectorized with optional Numba acceleration
- Order processing: Batch aggregation instead of per-order iteration
- Delta computation: Vectorized numpy operations instead of apply()

### Known Limitations & TODOs for P4

**Not Yet Optimized:**
- Equity simulation loop in `simulate_equity()` (still uses per-day iteration)
- Cost calculation aggregation (already vectorized in `simulate_with_costs`)
- Signal generation and feature computation (user-provided, not in scope)

**Future Enhancements (P4):**
- **P4**: Batch Runner & Parallelization (run multiple backtests in parallel)
- Vectorize equity simulation loop (if feasible without breaking logic)
- Incremental updates (only recompute changed positions)
- Lazy evaluation (compute metrics on-demand)
- GPU acceleration (CuPy/Numba CUDA) for very large universes

### Compatibility

- **API Compatibility**: 100% - no changes to public APIs
- **Numerical Compatibility**: 100% - results match original within 1e-9 tolerance
- **Backward Compatibility**: Full - all existing code continues to work
- **Optional Dependencies**: Numba is optional, graceful fallback to pandas

---

## 11. Future Enhancements (Post-P3)

