# N√§chste Schritte - Empfehlungen

**Datum:** 2025-12-09  
**Status:** Downloads laufen, 22/24 AI-Tech Symbole erfolgreich

---

## ‚úÖ Was bereits erledigt ist

1. **Twelve Data Integration**
   - ‚úÖ `TwelveDataPriceDataSource` implementiert
   - ‚úÖ Download-Skript erweitert (`--provider twelve_data`)
   - ‚úÖ Settings erweitert (`twelve_data_api_key`)
   - ‚úÖ Factory-Funktion erweitert

2. **Downloads**
   - ‚úÖ Macro-ETFs: 10/10 vollst√§ndig
   - ‚úÖ AI-Tech: 22/24 (92%)
   - ‚è≥ Andere Universen: Downloads laufen

3. **Tools erstellt**
   - ‚úÖ `check_data_completeness.py` - Vollst√§ndigkeitspr√ºfung
   - ‚úÖ `test_problem_symbols.ps1` - Problem-Symbole testen
   - ‚úÖ `setup_pipeline_integration.ps1` - Pipeline-Integration vorbereiten

---

## üîÑ Aktuelle Aufgaben (laufend)

### 1. Downloads abschlie√üen
**Status:** L√§uft im Hintergrund

**Aktionen:**
- Warten auf Abschluss der laufenden Downloads
- Fehlende Symbole identifizieren
- Problem-Symbole dokumentieren

**Erwartete Zeit:** ~15-20 Minuten f√ºr alle Universen

---

## üìã N√§chste Schritte (Priorit√§t)

### 1. Vollst√§ndigkeitspr√ºfung (Hoch)
**Ziel:** Alle heruntergeladenen Daten validieren

```powershell
# Alle Universen pr√ºfen
.\.venv\Scripts\python.exe scripts/check_data_completeness.py `
  --all-universes `
  --target-root "F:\Python_Projekt\Aktienger√ºst\datensammlungen\altdaten\stand 3-12-2025" `
  --interval 1d `
  --expected-start 2000-01-01 `
  --expected-end 2025-12-03
```

**Erwartetes Ergebnis:**
- Liste aller fehlenden Symbole
- Qualit√§tsbericht (Zeilen, Datumsbereiche, Spalten)
- Identifikation von Problem-Symbolen

---

### 2. Problem-Symbole dokumentieren (Mittel)
**Ziel:** Nicht verf√ºgbare Symbole identifizieren und dokumentieren

**Bekannte Problem-Symbole:**
- `IOS.DE` - Twelve Data: "symbol invalid"
- `SMHN.DE` - Twelve Data: "symbol invalid"
- `BAVA.CO` - Noch zu testen
- `EUZ.DE` - Noch zu testen

**Aktionen:**
1. Alle fehlenden Symbole einzeln testen
2. Alternative Ticker-Formate pr√ºfen (z.B. "IOS" statt "IOS.DE")
3. Dokumentieren, welche Symbole nicht verf√ºgbar sind
4. Entscheidung: Aus Universe entfernen oder sp√§ter mit anderem Provider nachladen

---

### 3. Pipeline-Integration (Hoch)
**Ziel:** Heruntergeladene Daten in die Pipeline integrieren

```powershell
# Environment-Variable setzen
$env:ASSEMBLED_LOCAL_DATA_ROOT = "F:\Python_Projekt\Aktienger√ºst\datensammlungen\altdaten\stand 3-12-2025"

# Oder Setup-Skript nutzen
.\scripts\setup_pipeline_integration.ps1
```

**Tests:**
1. **Datenladen testen:**
   ```python
   from src.assembled_core.data.data_source import get_price_data_source
   from src.assembled_core.config.settings import Settings
   
   settings = Settings()
   settings.local_data_root = Path("F:/Python_Projekt/Aktienger√ºst/datensammlungen/altdaten/stand 3-12-2025")
   ds = get_price_data_source(settings, "local")
   df = ds.get_history(["SPY"], "2010-01-01", "2025-12-03", "1d")
   print(f"Loaded {len(df)} rows for SPY")
   ```

2. **Backtest testen:**
   ```powershell
   python scripts/cli.py backtest `
     --freq 1d `
     --symbols-file config/macro_world_etfs_tickers.txt `
     --start-date 2010-01-01 `
     --end-date 2025-12-03
   ```

3. **Factor-Report testen:**
   ```powershell
   python scripts/cli.py factor_report `
     --freq 1d `
     --symbols-file config/macro_world_etfs_tickers.txt `
     --start-date 2010-01-01 `
     --end-date 2025-12-03 `
     --factor-set core `
     --fwd-horizon-days 5
   ```

---

### 4. Datenqualit√§t optimieren (Mittel)
**Ziel:** Zeitraum auf 2000-2025 erweitern (aktuell: 2010-2025)

**Problem:** Twelve Data Free-Tier limitiert auf 5000 Zeilen pro Request

**L√∂sungen:**
1. **Mehrere Requests:** Zeitraum in Chunks aufteilen (z.B. 2000-2010, 2010-2020, 2020-2025)
2. **Starter-Plan:** $9.99/Monat f√ºr mehr Daten
3. **Aktueller Stand akzeptieren:** 2010-2025 ist f√ºr viele Analysen ausreichend

**Empfehlung:** Erstmal mit 2010-2025 arbeiten, sp√§ter auf Starter-Plan upgraden wenn n√∂tig

---

### 5. Dokumentation aktualisieren (Niedrig)
**Ziel:** Download-Workflow und Provider-Strategie dokumentieren

**Aktionen:**
1. `docs/DATA_DOWNLOAD_STATUS.md` aktualisieren (bereits erstellt)
2. `README.md` erweitern mit Download-Anleitung
3. Provider-Vergleich dokumentieren
4. Problem-Symbole-Liste pflegen

---

## üéØ Empfohlene Reihenfolge

### Sofort (heute):
1. ‚úÖ Downloads abschlie√üen lassen
2. ‚úÖ Vollst√§ndigkeitspr√ºfung durchf√ºhren
3. ‚úÖ Problem-Symbole dokumentieren

### Diese Woche:
1. Pipeline-Integration testen
2. Backtest mit neuen Daten ausf√ºhren
3. Factor-Report mit Phase A Faktoren testen

### N√§chste Woche:
1. Datenqualit√§t optimieren (Zeitraum erweitern)
2. Finnhub f√ºr Alt-Daten vorbereiten
3. Dokumentation finalisieren

---

## üìä Erwartete Ergebnisse

### Nach Vollst√§ndigkeitspr√ºfung:
- **~50-55/59 Symbole** erfolgreich (85-93%)
- **~4-9 Problem-Symbole** (meist europ√§ische Ticker)
- **Qualit√§tsbericht** f√ºr alle Universen

### Nach Pipeline-Integration:
- Backtest l√§uft mit neuen Daten
- Factor-Report funktioniert
- Alle Phase A Faktoren k√∂nnen berechnet werden

---

## ‚ö†Ô∏è Bekannte Probleme

1. **Twelve Data Free-Tier Limits:**
   - 8 Calls/Minute ‚Üí 8 Sekunden Pause zwischen Calls
   - 800 Calls/Tag ‚Üí ~100 Symbole pro Tag m√∂glich
   - 5000 Zeilen max ‚Üí Zeitraum 2010-2025 statt 2000-2025

2. **Europ√§ische Ticker:**
   - Viele .DE, .CO, .AX Ticker nicht bei Twelve Data verf√ºgbar
   - L√∂sung: Sp√§ter mit Finnhub oder anderem Provider nachladen

3. **Rate-Limits:**
   - Bei zu schnellen Downloads: "API credits exhausted"
   - L√∂sung: L√§ngere Pausen (8+ Sekunden) zwischen Calls

---

## üí° Tipps

1. **Downloads √ºberwachen:**
   - Log-Dateien in `logs/` pr√ºfen
   - Status regelm√§√üig mit `check_data_completeness.py` pr√ºfen

2. **Problem-Symbole:**
   - Nicht verf√ºgbare Symbole aus Universe entfernen oder markieren
   - Sp√§ter mit alternativem Provider nachladen

3. **Pipeline-Tests:**
   - Zuerst mit kleinen Universen testen (z.B. Macro-ETFs)
   - Dann auf gr√∂√üere Universen erweitern

